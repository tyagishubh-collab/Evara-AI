# Pathfinder â€” AI Navigation Assistant for Blind Users

**Offline-first, privacy-preserving navigation system with obstacle detection, haptic feedback, and turn-by-turn guidance.**

## ğŸ¯ Features

### P0 (MVP - Works Offline)
- âœ… **Obstacle Detection**: YOLOv8 camera-based detection
- âœ… **Distance Sensing**: Ultrasonic/ToF sensor fusion
- âœ… **Sector Analysis**: Left/center/right free-space detection
- âœ… **TTS Guidance**: Offline text-to-speech prompts
- âœ… **Haptic Feedback**: Vibration motors mapped to obstacle direction
- âœ… **Wake Word**: Offline STT for voice commands

### P1 (Enhanced - Modules Ready)
- ğŸš§ Crosswalk & traffic light detection
- ğŸš§ Sidewalk vs road classification
- ğŸš§ Indoor navigation via AprilTags/ArUco
- ğŸš§ Outdoor navigation via offline OSM
- ğŸš§ Safe path planner (avoids stairs/curbs)
- ğŸš§ SOS emergency (triple-press with GPS)

### P2 (Future)
- ğŸ“‹ Personalized routes & landmarks
- ğŸ“‹ On-device incremental learning
- ğŸ“‹ Multi-modal sensor fusion

## ğŸ› ï¸ Hardware Requirements

### Minimum (MVP - Development on PC)
- Webcam or USB camera
- Modern PC/Mac with Python 3.8+

### Recommended (Full System - Raspberry Pi)
- Raspberry Pi 4/5 (4GB+ RAM)
- USB webcam or Raspberry Pi Camera Module
- HC-SR04 ultrasonic sensor (or VL53L1X ToF)
- Vibration motors (3x) or ESP32 BLE band
- Optional: Luxonis OAK-D (built-in depth)
- Optional: MPU-6050 IMU
- Optional: u-blox GPS module

## ğŸ“¦ Installation

### 1. Clone Repository
```bash
git clone https://github.com/yourusername/Evara-AI.git
cd Evara-AI
```

### 2. Create Virtual Environment
```bash
python -m venv venv

# Windows
venv\Scripts\activate

# macOS/Linux
source venv/bin/activate
```

### 3. Install Dependencies
```bash
pip install -r requirements.txt
```

### 4. Download Models (Optional)
YOLOv8 will auto-download on first run. For offline STT:
```bash
# Download Vosk model
wget https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip
unzip vosk-model-small-en-us-0.15.zip -d models/
```

### 5. Configure Environment
```bash
cp .env.example .env
# Edit .env with your settings (GPIO pins, API keys, etc.)
```

## ğŸš€ Usage

### Run Pathfinder (MVP - Vision Only)
```bash
python -m src.main
```

Press **ESC** or **Q** to exit.

### Run Original VisualAid Server (Optional)
```bash
python app.py
```

## ğŸ”§ Configuration

Edit `.env` to configure:
- **Vision**: Model path, confidence threshold
- **Sensors**: GPIO pins for ultrasonic, IMU, GPS
- **Safety**: Distance thresholds, battery, thermal limits
- **TTS/STT**: Engine preferences
- **Optional**: Gemini API key for enhanced narration

## ğŸ›¡ï¸ Security

**âš ï¸ CRITICAL: The exposed API key has been fixed and must be rotated immediately.**

### Steps Taken
1. âœ… Fixed `app.py` to use environment variable
2. âœ… Created `.env.example` template
3. âœ… Updated `.gitignore` to prevent future leaks
4. âœ… Added pre-commit hooks with `gitleaks`

### Next Steps (MUST DO)
1. **Rotate the exposed Gemini API key**:
   - Go to Google Cloud Console
   - Delete the exposed key: `AIzaSyB5d8HhskPM-MIuVMXqoQWEO4X6xVvJLnQ`
   - Generate a new key
   - Add to `.env` file (never commit)

2. **Install pre-commit hooks**:
   ```bash
   pip install pre-commit
   pre-commit install
   pre-commit run --all-files
   ```

3. **Clean git history** (if key was committed):
   ```bash
   # Use git-filter-repo or BFG Repo-Cleaner
   # Force push after cleaning (coordinate with team)
   ```

## ğŸ“ Project Structure

```
Evara-AI/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py              # Main entry point
â”‚   â”œâ”€â”€ config.py            # Configuration management
â”‚   â”œâ”€â”€ perception/
â”‚   â”‚   â”œâ”€â”€ detector.py      # YOLOv8 detection
â”‚   â”‚   â””â”€â”€ depth.py         # Depth estimation (ready)
â”‚   â”œâ”€â”€ sensors/
â”‚   â”‚   â”œâ”€â”€ ultrasonic.py    # HC-SR04 (ready)
â”‚   â”‚   â”œâ”€â”€ imu.py           # MPU-6050 (ready)
â”‚   â”‚   â””â”€â”€ gps.py           # u-blox GPS (ready)
â”‚   â”œâ”€â”€ fusion/
â”‚   â”‚   â””â”€â”€ occupancy.py     # Sensor fusion
â”‚   â”œâ”€â”€ navigation/
â”‚   â”‚   â”œâ”€â”€ map.py           # OSM offline maps (ready)
â”‚   â”‚   â”œâ”€â”€ planner.py       # A* path planning (ready)
â”‚   â”‚   â””â”€â”€ guidance.py      # Turn-by-turn (ready)
â”‚   â”œâ”€â”€ i_o/
â”‚   â”‚   â”œâ”€â”€ tts.py           # Text-to-speech (ready)
â”‚   â”‚   â”œâ”€â”€ stt.py           # Speech-to-text (ready)
â”‚   â”‚   â””â”€â”€ haptics.py       # Vibration control (ready)
â”‚   â””â”€â”€ safety/
â”‚       â””â”€â”€ watchdog.py      # Battery/thermal/SOS (ready)
â”œâ”€â”€ app.py                   # Original FastAPI server
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ .pre-commit-config.yaml
â””â”€â”€ README.md
```

## âš ï¸ Safety Disclaimer

**This is an assistive aid, not a replacement for:**
- White cane
- Guide dog
- Professional orientation & mobility training

Always use in conjunction with established mobility aids and training.

## ğŸ¤ Contributing

1. Install pre-commit hooks: `pre-commit install`
2. Follow offline-first, privacy-preserving principles
3. Test on actual hardware (Raspberry Pi) when possible
4. Document GPIO pin assignments
5. Never commit secrets or API keys

See `CONTRIBUTING.md` for details.

## ğŸ§ª Development Status

### Currently Implemented
- âœ… Core configuration system
- âœ… YOLOv8 object detection
- âœ… Sensor fusion (vision + ultrasonic)
- âœ… Sector-based occupancy mapping
- âœ… Safe direction guidance
- âœ… Debug visualization

### Ready to Use (Modules Created)
- Depth estimation (MiDaS/OAK-D)
- Hardware sensors (ultrasonic, IMU, GPS)
- TTS/STT engines
- Haptic feedback
- Safety watchdog
- Navigation (map, planner, guidance)

### To Integrate
Simply uncomment imports in `src/main.py` and initialize the modules.

## ğŸ“„ License

[Your License Here]

## ğŸ™ Acknowledgments

- YOLOv8 by Ultralytics
- Vosk for offline STT
- OpenStreetMap for map data
- Pre-commit hooks for security

## ğŸ“§ Support

For hardware integration questions or bug reports, open an issue on GitHub.

---

**Built with â¤ï¸ for accessibility and independence**
